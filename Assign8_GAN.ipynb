{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L_NeJ6C_yyjv"
   },
   "source": [
    "# MIS 583 Assignment #8: Generative Adversarial Network(GAN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oSwr9MgZogRZ"
   },
   "source": [
    "Before we start, please put your name and ID in following format  \n",
    ": LASTNAME Firstname, ?000000000   //   e.g.) 陳耀融, M094020099\n",
    "\n",
    "**Your Answer:**   \n",
    "Hi I'm XXX, XXXXXXXXXX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hHDyU3UQbPCW"
   },
   "source": [
    "This tutorial was originally written by [Nathan Inkawhich](https://github.com/inkawhich) for PyTorch Offical Tutorial.  \n",
    "This version has been adapted by [Yao-Rong Chen](https://github.com/teacher144123/) for NSYSU MIS583 assigment.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FL04i4f_5hQj"
   },
   "source": [
    "Refrence:\n",
    "- DCGAN - PyTorch offical tutorial  \n",
    "https://pytorch.org/tutorials/beginner/dcgan_faces_tutorial.html\n",
    "- Self-Normalizing Neural Networks  \n",
    "https://arxiv.org/abs/1706.02515\n",
    "- Relativistic GAN  \n",
    "https://arxiv.org/abs/1807.00734\n",
    "\n",
    "Tutorial or Source code:\n",
    "- SELU make fnns great again  \n",
    "https://towardsdatascience.com/selu-make-fnns-great-again-snn-8d61526802a9\n",
    "- Relativistic GAN offical GitHub **(important)**  \n",
    "https://github.com/AlexiaJM/RelativisticGAN\n",
    "- third party Relativistic repo - ririw  \n",
    "https://github.com/ririw/Relativistic-GAN/blob/master/relgan/trainer.py\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B_MoQiztpxcK"
   },
   "source": [
    "## Generative Adversarial Network(GAN)\n",
    "\n",
    "\"GAN and its variations that are now being proposed is the most interesting idea in the last 10 years in ML, in my opinion.\" LeCun has ever said.\n",
    "\n",
    "GAN was designed by Ian Goodfellow in 2014 is an approach contains generator and discirminator.\n",
    "\n",
    "Though originally proposed as a form of generative model for **unsupervised learning**, GANs have also proven useful for **semi-supervised learning**, **fully supervised learning** and **reinforcement learning**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EM9sRPGzhMYT"
   },
   "source": [
    "### What is GAN\n",
    "\n",
    "GAN is a mixed architecture contains generator and discirminator. They have different goals.  \n",
    "- Generator generate fake images from latent code, Discriminator classify images into categories.\n",
    "- The primary goal of Generator is fool the discriminator, make loss of discriminator maximum.\n",
    "- In contrast, the main goal of discriminator is correctly classify whether a image is real(from original dataset) or fake(made by generator).\n",
    "\n",
    "\n",
    "In this tutorial, we will cover:\n",
    "- GAN\n",
    "- DCGAN\n",
    "- SELU\n",
    "- Relativistic GAN\n",
    "- or any advanced GAN you interested in\n",
    "\n",
    "This assignment will walk you through implementing a DCGAN using the CelebA dataset in PyTorch and apply some advanced modifications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "giUId1Naqacs"
   },
   "source": [
    "##  Versions of used packages\n",
    "\n",
    "We will check PyTorch version to make sure everything work properly.\n",
    "\n",
    "I use `python 3.8.11`, `torch==1.8.2` and `torchvision==0.9.2`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "brBNrpGCCSPJ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python 3.9.7 (default, Sep 16 2021, 16:59:28) [MSC v.1916 64 bit (AMD64)]\n",
      "torch 1.10.1+cu113\n",
      "torchvision 0.11.2+cu113\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import torch\n",
    "import torchvision\n",
    "print('python', sys.version.split('\\n')[0])\n",
    "print('torch', torch.__version__)\n",
    "print('torchvision', torchvision.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OhdbdJOsrbxL"
   },
   "source": [
    "## Prepare Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JCRMPiUMjdlJ"
   },
   "source": [
    "We use [CelebA Dataset](http://mmlab.ie.cuhk.edu.hk/projects/CelebA.html) dataset from cuhk university(The Chinese University of Hong Kong).\n",
    "\n",
    "https://drive.google.com/file/d/1CYHiydUBHbnTKmZnL7M7oT_FArKMeSo-/view?usp=sharing\n",
    "\n",
    "**Abstrct**  \n",
    "\n",
    "CelebFaces Attributes Dataset (CelebA) is a large-scale face attributes dataset with more than 200K celebrity images, each with 40 attribute annotations.  \n",
    "\n",
    "The images in this dataset cover large pose variations and background clutter. CelebA has large diversities, large quantities, and rich annotations, including\n",
    "\n",
    "- 10,177 number of identities,\n",
    "- 202,599 number of face images, and\n",
    "- 5 landmark locations, 40 binary attributes annotations per image.\n",
    "- Original Size: 218x178"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IiMThsYeDa2O"
   },
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "## How to Get Data\n",
    "\n",
    "Please open the file `celeba_dataset.zip`, creat shortcut to your Google Drive.\n",
    "\n",
    "1. open [LINK of Google Drive](https://drive.google.com/file/d/1CYHiydUBHbnTKmZnL7M7oT_FArKMeSo-/view?usp=sharing)\n",
    "2. Click \"Add shortcut to Drive\" in the top-right corner.\n",
    "3. Select the location where you want to place the shortcut.\n",
    "4. Click Add shortcut.\n",
    "\n",
    "After above procedures, we have a shortcut of zip file of dataset.  \n",
    "We can access this in colab after granting the permission of Google Drive.\n",
    "\n",
    "---\n",
    "\n",
    "請先到共用雲端硬碟將檔案 `twitter_sentiment.zip`，建立捷徑到自己的雲端硬碟中。\n",
    "\n",
    "> 操作步驟\n",
    "1. 點開雲端[連結](https://drive.google.com/file/d/1CYHiydUBHbnTKmZnL7M7oT_FArKMeSo-/view?usp=sharing)\n",
    "2. 點選右上角「新增雲端硬碟捷徑」\n",
    "3. 點選「我的雲端硬碟」\n",
    "4. 點選「新增捷徑」\n",
    "\n",
    "完成以上流程會在你的雲端硬碟中建立一個檔案的捷徑，接著我們在colab中取得權限即可使用。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jBY0b6zxI0r9"
   },
   "source": [
    "1. Executing the below code which will provide you with an authentication link\n",
    "2. Open the link\n",
    "3. Choose the Google account whose Drive you want to mount\n",
    "4. Allow Google Drive Stream access to your Google Account\n",
    "5. Copy the code displayed, paste it in the text box as shown below, and press Enter\n",
    "![](https://i1.wp.com/neptune.ai/wp-content/uploads/colab-code-copy.png?resize=512%2C102&ssl=1)\n",
    "\n",
    "Finish!\n",
    "\n",
    "---\n",
    "\n",
    "執行此段後點選出現的連結，允許授權後，複製授權碼，貼在空格中後按下ENTER，即完成與雲端硬碟連結。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cCXepUIVe5iJ"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cqO8DiB6VRQZ"
   },
   "source": [
    "### Unzip Data\n",
    "\n",
    "解壓縮 `celeba_dataset.zip` 後可以發現裡面有一個資料夾和五個 .txt 檔。\n",
    "\n",
    "- `img_align_celeba`: contain 202599 images.\n",
    "- `list_eval_partition`: image id partition for train, val and test.  \n",
    "- ...\n",
    "\n",
    "注意: 若有另外設定存放在雲端硬碟中的路徑，請記得本處路徑也須做更動。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aGCZQxZSfONu"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gVbtJxl6rc3t"
   },
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "s1N1OXuLvs3q"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.utils.data as data\n",
    "from torchvision import transforms, datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MxOYcxZwpBG5"
   },
   "source": [
    "We can load it just use torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "CsUl31ozvs3q"
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Dataset not found or corrupted. You can use download=True to download it",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_18240/3877545666.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mdataset_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdataset_test\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m \u001b[0mdataset_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdataset_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_18240/3877545666.py\u001b[0m in \u001b[0;36mget_data\u001b[1;34m()\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[1;31m# dataset_train = get_celeba('hw3/celeba_dataset/img_align_celebra', 'train',)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[1;31m# dataset_test = get_celeba('hw3/celeba_dataset/list_eval_partition', 'test')\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m     \u001b[0mdataset_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_celeba\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'./data'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'train'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m     \u001b[0mdataset_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_celeba\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'./data'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'test'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'train'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_18240/3877545666.py\u001b[0m in \u001b[0;36mget_celeba\u001b[1;34m(root, split, download)\u001b[0m\n\u001b[0;32m      6\u001b[0m         \u001b[0mtransforms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mNormalize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0.5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     ])\n\u001b[1;32m----> 8\u001b[1;33m     return datasets.CelebA(root, split=split, transform=transform,\n\u001b[0m\u001b[0;32m      9\u001b[0m         download=download)\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torchvision\\datasets\\celeba.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, root, split, target_type, transform, target_transform, download)\u001b[0m\n\u001b[0;32m     81\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     82\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_check_integrity\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 83\u001b[1;33m             raise RuntimeError('Dataset not found or corrupted.' +\n\u001b[0m\u001b[0;32m     84\u001b[0m                                ' You can use download=True to download it')\n\u001b[0;32m     85\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Dataset not found or corrupted. You can use download=True to download it"
     ]
    }
   ],
   "source": [
    "def get_celeba(root, split, download=False):\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize(64),\n",
    "        transforms.CenterCrop(64),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(0.5, 0.5),\n",
    "    ])\n",
    "    return datasets.CelebA(root, split=split, transform=transform,\n",
    "        download=download)\n",
    "\n",
    "def get_data():\n",
    "    # dataset_train = get_celeba('hw3/celeba_dataset/img_align_celebra', 'train',)\n",
    "    # dataset_test = get_celeba('hw3/celeba_dataset/list_eval_partition', 'test')\n",
    "    dataset_train = get_celeba('./data', 'train',)\n",
    "    dataset_test = get_celeba('./data', 'test')\n",
    "    print('train', len(dataset_train))\n",
    "    print('test', len(dataset_test))\n",
    "    return dataset_train, dataset_test\n",
    "\n",
    "dataset_train, dataset_test = get_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aEyxwKLBvs3r"
   },
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "img_size = 64\n",
    "train_data = data.DataLoader(dataset_train, batch_size=batch_size,\n",
    "                             drop_last=True, shuffle=True)\n",
    "test_data = data.DataLoader(dataset_test, batch_size=batch_size,\n",
    "                             drop_last=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V0rYc_eQqYOz"
   },
   "source": [
    "Let's sample some images and plot them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ct3M0E70vs3r"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "o-KvwRz0vs3r"
   },
   "outputs": [],
   "source": [
    "def plot_imgs(img, save_file=False, file_name='out'):\n",
    "    img = img.permute(1, 2, 0, 3).reshape(3, img_size, -1)\n",
    "    img = (img + 1) / 2\n",
    "    plt.figure(figsize=(20,10))\n",
    "    plt.imshow(img.permute(1, 2, 0))\n",
    "    plt.show()\n",
    "    if save_file:\n",
    "        plt.savefig(file_name)\n",
    "\n",
    "plot_imgs(next(iter(train_data))[0][:8])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8qf3U50kqfCA"
   },
   "source": [
    "## Build Model\n",
    "### DCGAN\n",
    "\n",
    "[DCGAN](https://arxiv.org/abs/1511.06434) (Deep Convolutional Generative Adversarial Network) was proposed in 2015.\n",
    "\n",
    "It use ConvTransposed(逆捲積) to build generator.\n",
    "\n",
    "Discriminator is normal classification using Convolution.\n",
    "\n",
    "![](https://i.imgur.com/NhERHis.png)\n",
    "\n",
    "**DCGAN has been a excellent base strucutre of GAN using convolution.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4Ss1Y2z8JaJ5"
   },
   "source": [
    "### SELU\n",
    "\n",
    "There are some disadvantages in DCGAN.  \n",
    "DCGAN can't be trained with high learn rate, it is easy to cause gradient vanishing.  \n",
    "Recently, some research change the loss function or activation to increase the stability of GAN.    \n",
    "**SELU** is a activation that was proved outperform many existing methods in Feed-Forward Neural Network.  \n",
    "High-resolution DCGAN paper mentioned BatchNorm + ReLU could be replaced with SELU.\n",
    "\n",
    "$$\n",
    "\\text{selu}(x) = \\lambda \\left\\{\n",
    "\\begin{aligned}\n",
    "    & x & if & x > 0 \\\\\n",
    "    & \\alpha e^x - \\alpha & if & x \\leq 0\n",
    "\\end{aligned}\n",
    "\\right. \\\\\n",
    "\\alpha = 1.6732, \\lambda = 1.0507\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "87QoYeq0afS4"
   },
   "source": [
    "### Relativistic GAN\n",
    "\n",
    "Relativistic GAN was proprosed in 2018.  \n",
    "Old GAN discriminator only tend to make fake data more similar to real data.  \n",
    "Ideal generator should also make real data fake(lost piece of original GAN) and make fake data real(original GAN).  \n",
    "It will increase stability and coverage into lower reconstruction loss.\n",
    "\n",
    "(A) 是如果用 divergence minimization 可以達到的效果，(B) 是目前 GAN 實際上用 loss 訓練，(C) 是 Relativistic GAN 的理想的優化\n",
    "\n",
    "![](https://i.imgur.com/JxiCksr.png)\n",
    "\n",
    "More detail please check out paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xtNczZTlvs3r"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "latent_dim = 100\n",
    "g_hidden = 128 # hidden dim of generator\n",
    "d_hidden = 128 # hidden dim of discriminator\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        # ---------- You Should Modify ----------\n",
    "        # TODO: add activation\n",
    "        self.main = nn.Sequential(\n",
    "            nn.ConvTranspose2d(latent_dim, g_hidden * 8,\n",
    "                kernel_size=4, stride=1, padding=0, bias=False),\n",
    "            # --> some regularization and activation\n",
    "            # shape is (g_hidden*8, 4, 4),\n",
    "\n",
    "            nn.ConvTranspose2d(g_hidden * 8, g_hidden * 4,\n",
    "                kernel_size=4, stride=2, padding=1, bias=False),\n",
    "            # --> some regularization and activation\n",
    "            # shape is (g_hidden*4, 8, 8),\n",
    "\n",
    "            nn.ConvTranspose2d(g_hidden * 4, g_hidden * 2,\n",
    "                kernel_size=4, stride=2, padding=1, bias=False),\n",
    "            # --> some regularization and activation\n",
    "            # shape is (g_hidden*2, 16, 16),\n",
    "\n",
    "            nn.ConvTranspose2d(g_hidden * 2, g_hidden,\n",
    "                kernel_size=4, stride=2, padding=1, bias=False),\n",
    "            # --> some regularization and activation\n",
    "            # shape is (g_hidden, 32, 32),\n",
    "\n",
    "            nn.ConvTranspose2d(g_hidden, 3,\n",
    "                kernel_size=4, stride=2, padding=1, bias=False),\n",
    "            nn.Tanh(),\n",
    "            # shape is (3, 64, 64),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        output = self.main(x)\n",
    "        return output\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        # ---------- You Should Modify ----------\n",
    "        # TODO: add activation\n",
    "        self.main = nn.Sequential(\n",
    "            nn.Conv2d(3, d_hidden,\n",
    "                kernel_size=4, stride=2, padding=1, bias=False),\n",
    "            # --> some regularization and activation\n",
    "            # shape is (d_hidden, 32, 32),\n",
    "\n",
    "            nn.Conv2d(d_hidden, d_hidden * 2,\n",
    "                kernel_size=4, stride=2, padding=1, bias=False),\n",
    "            # --> some regularization and activation\n",
    "            # shape is (d_hidden * 2, 16, 16),\n",
    "\n",
    "            nn.Conv2d(d_hidden * 2, d_hidden * 4,\n",
    "                kernel_size=4, stride=2, padding=1, bias=False),\n",
    "            # --> some regularization and activation\n",
    "            # shape is (d_hidden * 4, 8, 8),\n",
    "\n",
    "            nn.Conv2d(d_hidden * 4, d_hidden * 8,\n",
    "                kernel_size=4, stride=2, padding=1, bias=False),\n",
    "            # --> some regularization and activation\n",
    "            # shape is (d_hidden * 8, 4, 4),\n",
    "\n",
    "            nn.Conv2d(d_hidden * 8, 1,\n",
    "                kernel_size=4, stride=1, padding=0, bias=False),\n",
    "            # shape is (1, 1, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        output = self.main(x)\n",
    "        return output.view(-1, 1).squeeze(1)\n",
    "\n",
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "\n",
    "    # ---------- You Should Modify ----------\n",
    "    # TODO: correct weight initialization\n",
    "    if 'Conv' in classname:\n",
    "        torch.nn.init.normal_(m.weight, 0.0, 0.02)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        torch.nn.init.normal_(m.weight, 1.0, 0.02)\n",
    "        torch.nn.init.zeros_(m.bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A0Z6Z6YBLt6P"
   },
   "source": [
    "### Test Model\n",
    "Let's test our model output shape with simulated data input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Civ6Yry1vs3r"
   },
   "outputs": [],
   "source": [
    "def test_model():\n",
    "    net_d = Discriminator()\n",
    "    net_g = Generator()\n",
    "\n",
    "    x = torch.rand(16, latent_dim, 1, 1)\n",
    "\n",
    "    out = net_g(x)\n",
    "    print(out.shape)\n",
    "    out2 = net_d(out)\n",
    "    print(out2.shape)\n",
    "test_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8AbO7trGMClY"
   },
   "source": [
    "GAN is **computation consummed**.  \n",
    "Adjust your parameter depends on your resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vgP7UHcQvs3r"
   },
   "outputs": [],
   "source": [
    "# Hyper-Parameter setting\n",
    "lr = 2e-4\n",
    "beta1 = 0.5\n",
    "start_epoch = 1\n",
    "max_epoch = 3\n",
    "dataset = train_data\n",
    "# Suggestted setting\n",
    "# train_data run 3 epoch\n",
    "# test_data run 30 epoch\n",
    "\n",
    "import os\n",
    "os.makedirs('ckpts', exist_ok=True)\n",
    "os.makedirs('out_imgs', exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pNN7DoyOvs3s"
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda')\n",
    "\n",
    "net_g = Generator().to(device)\n",
    "net_g.apply(weights_init) # apply weight init\n",
    "\n",
    "net_d = Discriminator().to(device)\n",
    "net_d.apply(weights_init) # apply weight init\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "optim_d = torch.optim.Adam(net_d.parameters(), lr=lr, betas=(beta1, 0.99))\n",
    "optim_g = torch.optim.Adam(net_g.parameters(), lr=lr, betas=(beta1, 0.99))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p5Mhf27AMcU6"
   },
   "source": [
    "Make some fixed noise.  \n",
    "To see evolutions of our generator model.  \n",
    "And labels filled with 0 and 1 for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rA_EMyXXvs3s"
   },
   "outputs": [],
   "source": [
    "fixed_noise = torch.randn(batch_size, latent_dim, 1, 1, device=device)\n",
    "real_label = torch.ones(batch_size, device=device).float()\n",
    "fake_label = torch.zeros(batch_size, device=device).float()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pdIix7JtMsDh"
   },
   "source": [
    "### Training Loop\n",
    "Start training!\n",
    "\n",
    "Training section of DCGAN can be split into two update stage and three parts.\n",
    "\n",
    "**First**, we feed discirminator some true images and label 1.\n",
    "\n",
    "**Second**, discriminator is trained with fake images generated by generator model and label 0.\n",
    "\n",
    "Then, we do one discriminator optimizer step.\n",
    "\n",
    "**Third**, we will make generator make some fake images and throw them into discriminator with true label 1 try to fool the discriminator.\n",
    "\n",
    "Finally, generator optimizer update once. \n",
    "\n",
    "<font color=\"red\">**BUT, remember Relativitistic GAN have different updated stage**</font>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8-ClwL0bvs3s"
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "log_step = int(len(dataset) * 0.2)\n",
    "t = time.time()\n",
    "\n",
    "for epoch in range(start_epoch, max_epoch + 1):\n",
    "    loader = enumerate(dataset)\n",
    "    d_loss_list = []\n",
    "    g_loss_list = []\n",
    "    for i, (imgs, _) in loader:\n",
    "        loss_temp = {\n",
    "            'err_d': [],\n",
    "            'err_g': []\n",
    "        }\n",
    "        imgs = imgs.to(device)\n",
    "        \n",
    "        # ========== Update Discriminator ==========\n",
    "        # ---------- You Should Modify ----------\n",
    "        net_d.zero_grad()\n",
    "        \n",
    "        # throw real image into discirminator\n",
    "        ...\n",
    "\n",
    "        # generate fake image and throw into discirminator\n",
    "        noise = torch.randn(batch_size, latent_dim, 1, 1, device=device)\n",
    "        fake_imgs = net_g(noise)\n",
    "        ...\n",
    "        \n",
    "        # calculate loss\n",
    "        err_d = ...\n",
    "        err_d.backward()\n",
    "        loss_temp['err_d'].append(err_d.item())\n",
    "        optim_d.step()\n",
    "        \n",
    "        # ========== Update Generator ==========\n",
    "        # ---------- You Should Modify ----------\n",
    "        net_g.zero_grad()\n",
    "        \n",
    "        # throw real image into discirminator\n",
    "        ...\n",
    "        \n",
    "        # throw fake image into discirminator\n",
    "        # noise = torch.randn(batch_size, latent_dim, 1, 1, device=device)\n",
    "        # fake_imgs = net_g(noise)\n",
    "        ...\n",
    "        \n",
    "        # ---------- You Should Modify ----------\n",
    "        err_g = ...\n",
    "        err_g.backward()\n",
    "        loss_temp['err_g'].append(err_g.item())\n",
    "        optim_g.step()\n",
    "        \n",
    "        if i % log_step == 0:\n",
    "            print('[{}/{} - {}/{}] Loss_D: {:.4f} Loss_G: {:.4f}'.format( # y_d: {:.4f} / {:.4f}\n",
    "                epoch, max_epoch, i, len(dataset),\n",
    "                err_d.item(), err_g.item(),# y_d_real, y_d_fake#, y_g_real, y_g_fake\n",
    "            ))\n",
    "\n",
    "            fix_fake_imgs = net_g(fixed_noise).cpu().detach()\n",
    "            plot_imgs(fix_fake_imgs[:8], True, file_name='out_imgs/e{:02}.png'.format(epoch))\n",
    "    \n",
    "    print('-' * 30)\n",
    "    l_d = sum(loss_temp['err_d']) / len(loss_temp['err_d'])\n",
    "    l_g = sum(loss_temp['err_g']) / len(loss_temp['err_g'])\n",
    "    d_loss_list.append(l_d)\n",
    "    g_loss_list.append(l_g)\n",
    "print('avg loss d', sum(d_loss_list) / len(d_loss_list))\n",
    "print('avg loss d', sum(g_loss_list) / len(g_loss_list))\n",
    "t = time.time() - t\n",
    "print('avg time', t / max_epoch, 'secs')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S1JkMUJnzLrU"
   },
   "source": [
    "## Your Task(Important)\n",
    "\n",
    "This task of this assignment is GAN modification (based on papaer and code).\n",
    "\n",
    "The basic of GAN is implementing a DCGAN architacture.\n",
    "\n",
    "For the advanced modification, You can do the SELU and the relativistic GAN.\n",
    "\n",
    "Tasks:\n",
    "- Use SELU as the activation of model, fill out correct weight initialization\n",
    "- Replace loss function with Repativistic GAN (RSGAN or RaSGAN are good), please read [offical GitHub](https://github.com/AlexiaJM/RelativisticGAN#to-add-relativism-to-your-own-gans-in-pytorch-you-can-use-pieces-of-code-from-below) for more details.\n",
    "- Any finding, comparison of multiple methods. **Complete description (not just comparison of experiment number)**\n",
    "- Any other advanced GAN is allowed (WGAN, gradient penalty, AAE...) (You can do this instead of SELU or Relativistic GAN parts)\n",
    "\n",
    "### Points for scoring\n",
    "- Full DCGAN, clear description >> **40 points**\n",
    "- Add SELU activation, Relativistic GAN loss function fail, unclear description **60 points**\n",
    "- Add SELU activation + Relativistic GAN loss function, clear description **100 points**\n",
    "- WGAN + Relativistic GAN loss + gradient penalty, clear description **100+ points**\n",
    "\n",
    "---\n",
    "\n",
    "**這段很重要打中文**  \n",
    "\n",
    "這個 lab 需要你們對 GAN 做出改善（根據論文、網路上的 code），主要是照 DCGAN 的框架下改動，最基本的是將 DCGAN 實現出來\n",
    "\n",
    "主要修改的方向，我有提供 SELU 和 relativistic GAN 兩個方向，你可以實作這兩個，可以得到共 50 分的配分，以下有一些不同的選項可以選擇，你會拿到不同的分數\n",
    "\n",
    "任務：\n",
    "- 用 SELU 當作 model 的 activation，並補上正確的 權重初始化方法（weight initialization) \n",
    "- 修改 loss function，用 Relativistic GAN 的方法(RSGAN 或 RaSGAN 都可以)，詳情請務必參考[官方的 GitHub Code](https://github.com/AlexiaJM/RelativisticGAN#to-add-relativism-to-your-own-gans-in-pytorch-you-can-use-pieces-of-code-from-below) \n",
    "- [必要] 你的任何發現、數個方法的比較或**詳細的說明**\n",
    "- **任何** GAN 的改進也非常歡迎 (WGAN, gradient penalty, AAE...) **(可以取代上面的任意分數)**\n",
    "\n",
    "請在這裡說明你要修改的項目：DCGAN model + Relativistic GAN loss\n",
    "\n",
    "### 我應該做多少努力\n",
    "- 假設你補上 DCGAN 的 activation + loss function，加上清楚說明 >> **25 分**\n",
    "- 修改成 SELU activation，Relativistic GAN loss function 失敗，未能清楚說明 >> **35 分**\n",
    "- 修改成 SELU activation + Relativistic GAN loss，清楚說明 >> **50 分**\n",
    "- 把架構改成 WGAN + Relativistic GAN loss，加上 gradient penalty，清楚說明 >> **60 分**"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Assign8_GAN.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
