{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hLcWRu-jp7gM"
   },
   "source": [
    "# MIS 583 Assignment 3: Flower Classfication"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oSwr9MgZogRZ"
   },
   "source": [
    "Before we start, please put your name and ID in following format: <br>\n",
    "NAME, ?000000000   e.g. 翁聖淇, M094020005\n",
    "\n",
    "**Your Answer:**   \n",
    "Hi I'm 莊明輯, M104020053"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B_MoQiztpxcK"
   },
   "source": [
    "## Flower Classification\n",
    "\n",
    "Image classification is a core and fundamental task in computer vision.\n",
    "\n",
    "In the assignment, you will implement a multi-class image classifier to recognize flowers.\n",
    "\n",
    "You will design and train a deep convolutional network from scratch to predict the class label of a flower image. This will help you gain experience with network design and get more familiar with PyTorch.\n",
    "\n",
    "**Please note that you’re not allowed to use a pre-trained model.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b1NQv3Ey3cVS"
   },
   "source": [
    "## Kaggle Competition\n",
    "Kaggle is an online community of data scientists and machine learning practitioners. Kaggle allows users to find and publish data sets, explore and build models in a web-based data-science environment, work with other data scientists and machine learning engineers, and enter competitions to solve data science challenges.\n",
    "\n",
    "This assignment use kaggle to calculate your grade.  \n",
    "Please use this [**LINK**](https://www.kaggle.com/c/mis583-2021-flower-classification) to join the competition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "giUId1Naqacs"
   },
   "source": [
    "##  Versions of used packages\n",
    "\n",
    "We will check PyTorch version to make sure everything work properly.\n",
    "\n",
    "I use `python 3.8.11`, `torch==1.8.2` and `torchvision==0.9.2`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "id": "Vuw-gNvjqcYe",
    "outputId": "681f5ca0-1a15-4611-a979-76665ef366c0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python 3.7.4 (default, Aug  9 2019, 18:34:13) [MSC v.1915 64 bit (AMD64)]\n",
      "torch 1.7.0\n",
      "torchvision 0.8.1\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import torch\n",
    "import torchvision\n",
    "print('python', sys.version.split('\\n')[0])\n",
    "print('torch', torch.__version__)\n",
    "print('torchvision', torchvision.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_FydjP4MTWuL"
   },
   "source": [
    "## Error handling\n",
    "\n",
    "**RuntimeError: CUDA out of memory...**\n",
    "> 發生原因可能為讀取的 batch 過大或是記憶體未釋放乾淨。若縮小 batch size 後仍出現錯誤請按照以下步驟重新載入 colab。\n",
    "> The reason of this error is over-sized batch_size or unreleased memory. If changing the batch_size smaller still cause this error. Please reload colab follow below instructions.\n",
    "\n",
    "1. Click 「Runtime」\n",
    "2. Click 「Factor reset runtime」\n",
    "3. Click 「Reconnect」\n",
    "4. Reload all chunk\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OhdbdJOsrbxL"
   },
   "source": [
    "# Prepare Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nPoSgD83teTQ"
   },
   "source": [
    "We use [Flowers Recognition](https://www.kaggle.com/alxmamaev/flowers-recognition) dataset.\n",
    "This is collected by Alexander Mamaev.\n",
    "\n",
    "**Abstrct**  \n",
    "\n",
    "This dataset contains 4317 flower images.  \n",
    "But, you can only get 1724(40%) images in this assignments.  \n",
    "**IMPORTANT: you CANNOT use any extra images.**\n",
    "\n",
    "The data collection is grabed from the data flicr, google images, yandex images.\n",
    "You can use this datastet to recognize plants from the photo.\n",
    "\n",
    "The pictures are divided into five classes: \n",
    "+ daisy\n",
    "+ tulip\n",
    "+ rose\n",
    "+ sunflower\n",
    "+ dandelion\n",
    "\n",
    "For each class there are about 800 photos. Photos are not high resolution, about 320x240 pixels. Photos are not reduced to a single size, they have different proportions!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IiMThsYeDa2O"
   },
   "source": [
    "## How to Get Data\n",
    "\n",
    "Please open the file `assignment_3_data_flowers_2021.zip`, creat shortcut to your Google Drive.\n",
    "\n",
    "1. open [LINK of Google Drive](https://drive.google.com/file/d/177A5hZiw8Ig2LlUlO-oozB89BBZZe2Gw)\n",
    "2. Click \"Add shortcut to Drive\" in the top-right corner.\n",
    "3. Select the location where you want to place the shortcut.\n",
    "4. Click Add shortcut.\n",
    "\n",
    "After above procedures, we have a shortcut of zip file of dataset.  \n",
    "We can access this in colab after granting the permission of Google Drive.\n",
    "\n",
    "---\n",
    "\n",
    "請先到共用雲端硬碟將檔案 `assignment_3_data_flowers_2021.zip`，建立捷徑到自己的雲端硬碟中。\n",
    "\n",
    "> 操作步驟\n",
    "1. 點開雲端[連結](https://drive.google.com/file/d/177A5hZiw8Ig2LlUlO-oozB89BBZZe2Gw)\n",
    "2. 點選右上角「新增雲端硬碟捷徑」\n",
    "3. 點選「我的雲端硬碟」\n",
    "4. 點選「新增捷徑」\n",
    "\n",
    "完成以上流程會在你的雲端硬碟中建立一個檔案的捷徑，接著我們在colab中取得權限即可使用。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jBY0b6zxI0r9"
   },
   "source": [
    "1. Executing the below code which will provide you with an authentication link\n",
    "2. Open the link\n",
    "3. Choose the Google account whose Drive you want to mount\n",
    "4. Allow Google Drive Stream access to your Google Account\n",
    "5. Copy the code displayed, paste it in the text box as shown below, and press Enter\n",
    "![](https://i1.wp.com/neptune.ai/wp-content/uploads/colab-code-copy.png?resize=512%2C102&ssl=1)\n",
    "\n",
    "Finish!\n",
    "\n",
    "---\n",
    "\n",
    "執行此段後點選出現的連結，允許授權後，複製授權碼，貼在空格中後按下ENTER，即完成與雲端硬碟連結。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cCXepUIVe5iJ"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cqO8DiB6VRQZ"
   },
   "source": [
    "## Unzip Data\n",
    "\n",
    "Unzip `assignment_3_data_flowers_2021.zip`, there are 2 folders and 3 csvs.\n",
    "\n",
    "- `train/`: contains 5 folders for 5 categories of flowers. Images of flowers inside them.\n",
    "- `test/`: unclassified images of testing set.\n",
    "- `train.csv`: file path and true label of training set.\n",
    "- `val.csv`: file path and true label of validation set.\n",
    "- `train.csv`: file paht of testing set.\n",
    "\n",
    "There are **1295 images in dataset_train.**  \n",
    "There are **431 images in dataset_test.**  \n",
    "There are **430 images in dataset_val.**  \n",
    "\n",
    "---\n",
    "\n",
    "解壓縮 `flower_data.zip` 後可以發現裡面有兩個資料夾和三個csv檔。\n",
    "\n",
    "+ `train` : 存有五個資料夾分別是五個種類的花，資料夾內為花的照片。\n",
    "+ `test` : 資料夾中為未分類之測試集照片。\n",
    "+ `train.csv` : 讀取 train data 的順序、路徑與圖片所屬花別。\n",
    "+ `val.csv` : 讀取 validate data 的順序、路徑與圖片所屬花別。\n",
    "+ `test.csv` : 讀取 test data 的順序、路徑。\n",
    "\n",
    "其中`train`的圖片  1295 張，`val` 的圖片 430 張，`test` 的圖片 431 張。\n",
    "\n",
    "注意: 若有另外設定存放在雲端硬碟中的路徑，請記得本處路徑也須做更動。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aGCZQxZSfONu"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "lYccv1qYzYUz"
   },
   "outputs": [],
   "source": [
    "data_folder = 'hw3/assignment_3_data_flowers_2021'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SYueKGdIHpho"
   },
   "source": [
    "## Loading the dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xpG2DxEqHFD-"
   },
   "source": [
    "### Custom dataset\n",
    "\n",
    "Build a classs inherit `torch.utils.data.Dataset`.  \n",
    "Implement `__init__`, `__getitem__` and `__len__` 3 functions.  \n",
    "\n",
    "Some operations could be there: setting location of dataset, the method of reading data, label of dataset or transform of dataset.\n",
    "\n",
    "See [torch.utils.data.Dataset](https://pytorch.org/docs/stable/data.html#torch.utils.data.Dataset) for more details\n",
    "\n",
    "---\n",
    "\n",
    "繼承自定義資料集的框架 `torch.utils.data.Dataset`，主要實現 `__getitem__()` 和 `__len__()` 這兩個方法。\n",
    "\n",
    "常使用來做到設定資料位址、設定讀取方式、子資料集的標籤和轉換條件...等。\n",
    "\n",
    "See [torch.utils.data.Dataset](https://pytorch.org/docs/stable/data.html#torch.utils.data.Dataset) for more details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "dwUE1E83_Iw8"
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import torch\n",
    "\n",
    "\n",
    "class FlowerData(torch.utils.data.Dataset):\n",
    "    def __init__(self, csv_file, mode='train', transform=None):\n",
    "        self.mode = mode # 'train', 'val' or 'test'\n",
    "        self.data_list = []\n",
    "        self.labels = []\n",
    "        self.transform = transform\n",
    "        \n",
    "        with open(f'{data_folder}/{csv_file}', newline='') as csvfile:\n",
    "            reader = csv.DictReader(csvfile)\n",
    "            for row in reader:\n",
    "                self.data_list.append(f\"{data_folder}/{row['file_path']}\")\n",
    "                if mode != 'test':\n",
    "                    self.labels.append(row['label'])\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        data = Image.open(self.data_list[index])\n",
    "        if self.transform is not None:\n",
    "            data = self.transform(data)\n",
    "        if self.mode == 'test':\n",
    "            return data\n",
    "        label = torch.tensor(int(self.labels[index]))\n",
    "\n",
    "        return data, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5tQGLuVWnA-b"
   },
   "source": [
    "### Data augmentation \n",
    "\n",
    "Data augmentation are techniques used to increase the amount of data by adding slightly modified copies of already existing data or newly created synthetic data from existing data.\n",
    "\n",
    "PyTorch use `torchvision.transforms` to do data augmentation.\n",
    "[You can see all function here.](https://pytorch.org/vision/stable/transforms.html)\n",
    "\n",
    "**NOTICE**: There are some operations may not be necessary for predict, so we should write one for train and one for others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "UIv1VyHXVNTo"
   },
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "# For TRAIN\n",
    "########################################################################\n",
    "#  TODO: use transforms.xxx method to do some data augmentation        #\n",
    "#  This one is for training, find the composition to get better result #\n",
    "########################################################################\n",
    "transforms_train = transforms.Compose([\n",
    "transforms.Resize((256,256)),\n",
    "transforms.RandomCrop((224,224)),\n",
    "transforms.RandomHorizontalFlip(p=0.5),\n",
    "transforms.RandomRotation(degrees=(-90,90)),\n",
    "transforms.ToTensor(),\n",
    "transforms.Normalize(mean=[0.485,0.456,0.406],std=[0.229,0.224,0.225]),\n",
    "])\n",
    "########################################################################\n",
    "#                           End of your code                           #\n",
    "########################################################################\n",
    "\n",
    "# For VAL, TEST\n",
    "########################################################################\n",
    "#  TODO: use transforms.xxx method to do some data augmentation        #\n",
    "#  This one is for validate and test,                                  #\n",
    "#  NOTICE some operation we usually not use in this part               #\n",
    "########################################################################\n",
    "transforms_test = transforms.Compose([\n",
    "transforms.Resize((256,256)),\n",
    "transforms.CenterCrop((224,224)),\n",
    "transforms.ToTensor(),\n",
    "transforms.Normalize(mean=[0.485,0.456,0.406],std=[0.229,0.224,0.225]),\n",
    "])\n",
    "########################################################################\n",
    "#                           End of your code                           #\n",
    "########################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7rYptn_YJlFX"
   },
   "source": [
    "### Instantiate dataset\n",
    "\n",
    "Let's instantiate three `FlowerData` class.\n",
    "+ dataset_train: for training.\n",
    "+ dataset_val: for validation.\n",
    "+ dataset_test: for tesing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "LKjsKyHhZZc6"
   },
   "outputs": [],
   "source": [
    "dataset_train = FlowerData('train.csv', mode='train', transform=transforms_train)\n",
    "dataset_val = FlowerData('val.csv', mode='val', transform=transforms_test)\n",
    "dataset_test = FlowerData('test.csv', mode='test', transform=transforms_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "id": "ZAeQQEEISCJJ",
    "outputId": "d8a3ee5c-72d2-430e-ff29-3c21a01c83c8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The first image's shape in dataset_train : torch.Size([3, 224, 224])\n",
      "There are 1295 images in dataset_train.\n"
     ]
    }
   ],
   "source": [
    "print(\"The first image's shape in dataset_train :\", dataset_train.__getitem__(0)[0].size())\n",
    "print(\"There are\", dataset_train.__len__(), \"images in dataset_train.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vORA6qkfIj1U"
   },
   "source": [
    "### `DataLoader`\n",
    "\n",
    "`torch.utils.data.DataLoader` define how to sample from `dataset` and some other function like:\n",
    "+ `shuffle` : set to `True` to have the data reshuffled at every epoch\n",
    "+ `batch_size` : how many samples per batch to load\n",
    "\n",
    "See [torch.utils.data.DataLoader](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader) for more details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "RmgA5nYT3XQZ"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "batch_size = 4\n",
    "num_workers = 0\n",
    "train_loader = DataLoader(dataset_train, batch_size=batch_size, num_workers=num_workers, shuffle=True)\n",
    "val_loader = DataLoader(dataset_val, batch_size=batch_size, num_workers=num_workers, shuffle=False)\n",
    "test_loader = DataLoader(dataset_test, batch_size=batch_size, num_workers=num_workers, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U4atwzT3aPi3"
   },
   "source": [
    "Finally! We have made all data prepared.  \n",
    "Let's go develop our model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "87KYcWknS95z"
   },
   "source": [
    "# Implement CNN using PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OH_4NKB9dsZ3"
   },
   "source": [
    "### Define a Convolutional Neural Network\n",
    "\n",
    "Try to design and train a deep convolutional network from scratch to predict the class label of a flower image. \n",
    "\n",
    "You can refer to last assignment about image_classifier, and try to go deep and use more method for better model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "S_HxxEfdO2Ss"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn \n",
    "import torch.nn.functional as F\n",
    "#resnet101=torchvision.models.resnet101(pretrained=True)\n",
    "\n",
    "class Your_CNN_Model(nn.Module): \n",
    "    def __init__(self): \n",
    "        super().__init__()\n",
    "        ########################################################################\n",
    "        #     TODO: use nn.xxx method to generate a CNN model part             #\n",
    "        ########################################################################\n",
    "        '''self.conv_layers = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(256, 512, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "        self.linear_layers = nn.Sequential(\n",
    "            nn.Linear(512*7*7, 4096),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout2d(0.5),\n",
    "            nn.Linear(4096,4096),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout2d(0.5),\n",
    "            nn.Linear(4096,5)\n",
    "        )'''\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=2),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "        \n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=2),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "        \n",
    "        self.layer3 = nn.Sequential(\n",
    "            nn.Conv2d(128, 256, kernel_size=5, stride=1, padding=2),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "        \n",
    "        self.layer4 = nn.Sequential(\n",
    "            nn.Conv2d(256, 512, kernel_size=5, stride=1, padding=2),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "        \n",
    "        self.layer5 = nn.Sequential(\n",
    "            nn.Conv2d(512, 1024, kernel_size=5, stride=1, padding=2),\n",
    "            nn.BatchNorm2d(1024),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "        \n",
    "        self.fc1 = nn.Linear(1024*7*7, 256)\n",
    "        self.fc2 = nn.Linear(256, 512)\n",
    "        self.fc3 = nn.Linear(512, 5)\n",
    "        self.conv1 = nn.Conv2d(3,6,5)\n",
    "        self.pool1 = nn.MaxPool2d(2,2)\n",
    "        self.conv2 = nn.Conv2d(6,16,5)\n",
    "        self.pool2 = nn.MaxPool2d(2,2)\n",
    "        '''self.fc1 = nn.Linear(16*53*53,120)\n",
    "        self.fc2 = nn.Linear(120,84)\n",
    "        self.fc3 = nn.Linear(84,5)'''\n",
    "        # You can change the variable name\n",
    "        ########################################################################\n",
    "        #                           End of your code                           #\n",
    "        ########################################################################\n",
    "\n",
    "    def forward(self, x): \n",
    "        if not isinstance(x, torch.Tensor):\n",
    "            x = torch.Tensor(x)\n",
    "        ########################################################################\n",
    "        #     TODO: forward your model and get output                          #\n",
    "        ########################################################################\n",
    "        '''x = self.conv_layers(x)\n",
    "        # flatten to prepare for the fully connected layers\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.linear_layers(x)'''\n",
    "        out = self.layer1(x)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.layer4(out)\n",
    "        out = self.layer5(out)\n",
    "        out = out.reshape(out.size(0), -1)\n",
    "        out = self.fc1(out)\n",
    "        out = F.dropout(out, training=self.training)\n",
    "        out = self.fc2(out)\n",
    "        out = F.dropout(out, training=self.training)\n",
    "        out = self.fc3(out)\n",
    "        \n",
    "        '''x = self.pool1(F.relu(self.conv1(x)))\n",
    "        x = self.pool2(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1,16*53*53)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)'''\n",
    "        ########################################################################\n",
    "        #                           End of your code                           #\n",
    "        ########################################################################\n",
    "        return F.log_softmax(out,dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "wOBCTZmbzYU4"
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda')\n",
    "# or\n",
    "# device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "TeWtAa1dzYU4"
   },
   "outputs": [],
   "source": [
    "model = Your_CNN_Model()\n",
    "model = model.to(device)\n",
    "#print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.3156, -1.7940, -1.6647, -1.6723, -1.6696],\n",
      "        [-1.2955, -1.5220, -1.7897, -2.4157, -1.3796],\n",
      "        [-1.6958, -1.3270, -1.7472, -2.4257, -1.2427],\n",
      "        [-1.3885, -1.7216, -1.8210, -1.6714, -1.5054],\n",
      "        [-2.2661, -1.4798, -1.3316, -2.1534, -1.2432],\n",
      "        [-1.2964, -1.7374, -1.6523, -1.8356, -1.6125],\n",
      "        [-1.2976, -1.4193, -1.5810, -2.1058, -1.8490],\n",
      "        [-1.4388, -2.1176, -1.5258, -2.1023, -1.1945],\n",
      "        [-1.3953, -1.7053, -1.9794, -1.7392, -1.3598],\n",
      "        [-1.2211, -1.4462, -1.6893, -2.3802, -1.6479],\n",
      "        [-1.4009, -1.0674, -2.4844, -2.0601, -1.6148],\n",
      "        [-1.4677, -1.8862, -1.9906, -1.8369, -1.1333],\n",
      "        [-1.4469, -1.9091, -1.5888, -1.4238, -1.7630],\n",
      "        [-2.0511, -1.7795, -2.3194, -2.6043, -0.6341],\n",
      "        [-1.7459, -1.4681, -2.4001, -1.6538, -1.1611],\n",
      "        [-1.9128, -1.4764, -1.4455, -1.6011, -1.6789],\n",
      "        [-0.9499, -1.3900, -2.0523, -2.2346, -2.0504],\n",
      "        [-1.6729, -1.3576, -2.2345, -1.7866, -1.2714],\n",
      "        [-1.2469, -1.9621, -1.5832, -2.3736, -1.2961],\n",
      "        [-1.5324, -1.6632, -2.1237, -1.7890, -1.1785],\n",
      "        [-2.1515, -1.6760, -2.0500, -2.1115, -0.8057],\n",
      "        [-1.5371, -1.4984, -1.7372, -2.4772, -1.1989],\n",
      "        [-1.6728, -1.6067, -1.5965, -2.2726, -1.1839],\n",
      "        [-1.9508, -1.1714, -2.3964, -1.2958, -1.6973],\n",
      "        [-1.6680, -1.6920, -2.1266, -1.7361, -1.1033],\n",
      "        [-1.7949, -1.2326, -1.7238, -2.3424, -1.3173],\n",
      "        [-1.7117, -1.6772, -1.7979, -1.9140, -1.1413],\n",
      "        [-1.8418, -1.1835, -1.6442, -1.8088, -1.7246],\n",
      "        [-1.6650, -1.8709, -1.5072, -1.7738, -1.3257],\n",
      "        [-1.3977, -1.8922, -1.5871, -1.7780, -1.4758],\n",
      "        [-1.9243, -1.4869, -1.5147, -2.0003, -1.2991],\n",
      "        [-1.6146, -1.0115, -2.3653, -2.7093, -1.2843]], device='cuda:0',\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "torch.Size([32, 5])\n",
      "tensor([0, 0, 4, 0, 4, 0, 0, 4, 4, 0, 1, 4, 3, 4, 4, 2, 0, 4, 0, 4, 4, 4, 4, 1,\n",
      "        4, 1, 4, 1, 4, 0, 4, 1], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "x = torch.rand(32, 3, 224, 224) # generate fake data\n",
    "x = x.to(device)\n",
    "out = model(x) # output of category and attribute\n",
    "print(out)\n",
    "print(out.shape)\n",
    "_,p=torch.max(out.data,1)\n",
    "print(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "83imqLRQ0v7M"
   },
   "source": [
    "We have made our model!  \n",
    "Next, PyTorch also provide many utility function(loss, optmizer...etc).  \n",
    "You can define them in one-line.\n",
    "### Define loss and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "IoePct00RIFY"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "################################################################################\n",
    "# TODO: Define loss and optmizer functions                                     #\n",
    "# Try any loss or optimizer function and learning rate to get better result    #\n",
    "# hint: torch.nn and torch.optim                                               #\n",
    "################################################################################\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "#params = model.parameters()\n",
    "#optimizer = torch.optim.Adam(params, lr=0.0001)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "################################################################################\n",
    "#                               End of your code                               #\n",
    "################################################################################\n",
    "criterion = criterion.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zle9KuFcbwMP"
   },
   "source": [
    "### Train the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bZFxE7Y9iLfl"
   },
   "source": [
    "#### Train function\n",
    "Let's define train function.  \n",
    "It will iterate input data 1 epoch and update model with optmizer.  \n",
    "Finally, calculate mean loss and total accuracy.\n",
    "\n",
    "Hint: [torch.max()](https://pytorch.org/docs/stable/generated/torch.max.html#torch-max) or [torch.argmax()](https://pytorch.org/docs/stable/generated/torch.argmax.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "VM93brDshO6E"
   },
   "outputs": [],
   "source": [
    "def train(input_data, model, criterion, optimizer):\n",
    "    '''\n",
    "    Argement:\n",
    "    input_data -- iterable data, typr torch.utils.data.Dataloader is prefer\n",
    "    model -- nn.Module, model contain forward to predict output\n",
    "    criterion -- loss function, used to evaluate goodness of model\n",
    "    optimizer -- optmizer function, method for weight updating\n",
    "    '''\n",
    "    model.train()\n",
    "    loss_list = []\n",
    "    total_count = 0\n",
    "    acc_count = 0\n",
    "    for images, labels in input_data:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        ########################################################################\n",
    "        # TODO: Forward, backward and optimize                                 #\n",
    "        # 1. zero the parameter gradients                                      #\n",
    "        # 2. process input through the network                                 #\n",
    "        # 3. compute the loss                                                  #\n",
    "        # 4. propagate gradients back into the network’s parameters            #\n",
    "        # 5. Update the weights of the network                                 #\n",
    "        ########################################################################\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        ########################################################################\n",
    "        #                           End of your code                           #\n",
    "        ########################################################################\n",
    "\n",
    "\n",
    "        ########################################################################\n",
    "        # TODO: Get the counts of correctly classified images                  #\n",
    "        # 1. get the model predicted result                                    #\n",
    "        # 2. sum the number of this batch predicted images                     #\n",
    "        # 3. sum the number of correctly classified                            #\n",
    "        # 4. save this batch's loss into loss_list                             #\n",
    "        # dimension of outputs: [batch_size, number of classes]                #\n",
    "        # Hint 1: use outputs.data to get no auto_grad                         #\n",
    "        # Hint 2: use torch.max()                                              #\n",
    "        ########################################################################\n",
    "        _, predicted = torch.max(outputs.data,1)\n",
    "        total_count += labels.size(0)\n",
    "        acc_count += (predicted == labels).sum().item()\n",
    "        loss_list.append(loss.item())\n",
    "        ########################################################################\n",
    "        #                           End of your code                           #\n",
    "        ########################################################################\n",
    "\n",
    "    # Compute this epoch accuracy and loss\n",
    "    acc = acc_count / total_count\n",
    "    loss = sum(loss_list) / len(loss_list)\n",
    "    return acc, loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KmDy1GTq_H2a"
   },
   "source": [
    "#### Validate function\n",
    "Next part is validate function.  \n",
    "It works as training function without optmizer and weight-updating part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "USzbBgGEoTRu"
   },
   "outputs": [],
   "source": [
    "def val(input_data, model, criterion):\n",
    "    model.eval()\n",
    "    \n",
    "    loss_list = []\n",
    "    total_count = 0\n",
    "    acc_count = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in input_data:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            ####################################################################\n",
    "            # TODO: Get the predicted result and loss                          #\n",
    "            # 1. process input through the network                             #\n",
    "            # 2. compute the loss                                              #\n",
    "            # 3. get the model predicted result                                #\n",
    "            # 4. get the counts of correctly classified images                 #\n",
    "            # 5. save this batch's loss into loss_list                         #\n",
    "            ####################################################################\n",
    "            outputs_val = model(images)\n",
    "            loss = criterion(outputs_val, labels)\n",
    "            \n",
    "            _, predicted = torch.max(outputs_val.data,1)\n",
    "            total_count += labels.size(0)\n",
    "            acc_count += (predicted == labels).sum().item()\n",
    "            loss_list.append(loss.item())\n",
    "            ####################################################################\n",
    "            #                         End of your code                         #\n",
    "            ####################################################################\n",
    "\n",
    "    acc = acc_count / total_count\n",
    "    loss = sum(loss_list) / len(loss_list)\n",
    "    return acc, loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "knXu74jCiuxP"
   },
   "source": [
    "#### Training in a loop\n",
    "Call train and test function in a loop.  \n",
    "Take a break and wait."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "rcVulKkFJRtI"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================== Epoch 2 ====================\n",
      "Train Acc: 0.263320 Train Loss: 10.887009\n",
      "  Val Acc: 0.369767   Val Loss: 16.802913\n",
      "==================== Epoch 4 ====================\n",
      "Train Acc: 0.309653 Train Loss: 3.250026\n",
      "  Val Acc: 0.339535   Val Loss: 2.294042\n",
      "==================== Epoch 6 ====================\n",
      "Train Acc: 0.336680 Train Loss: 1.790827\n",
      "  Val Acc: 0.353488   Val Loss: 1.835934\n",
      "==================== Epoch 8 ====================\n",
      "Train Acc: 0.350579 Train Loss: 1.500573\n",
      "  Val Acc: 0.348837   Val Loss: 1.508442\n",
      "==================== Epoch 10 ====================\n",
      "Train Acc: 0.418533 Train Loss: 1.399962\n",
      "  Val Acc: 0.434884   Val Loss: 1.387085\n",
      "==================== Epoch 12 ====================\n",
      "Train Acc: 0.433977 Train Loss: 1.317088\n",
      "  Val Acc: 0.481395   Val Loss: 1.382116\n",
      "==================== Epoch 14 ====================\n",
      "Train Acc: 0.450193 Train Loss: 1.254486\n",
      "  Val Acc: 0.476744   Val Loss: 1.290282\n",
      "==================== Epoch 16 ====================\n",
      "Train Acc: 0.470270 Train Loss: 1.222236\n",
      "  Val Acc: 0.506977   Val Loss: 1.325517\n",
      "==================== Epoch 18 ====================\n",
      "Train Acc: 0.466409 Train Loss: 1.240585\n",
      "  Val Acc: 0.490698   Val Loss: 1.313681\n",
      "==================== Epoch 20 ====================\n",
      "Train Acc: 0.477992 Train Loss: 1.194838\n",
      "  Val Acc: 0.479070   Val Loss: 1.228761\n",
      "==================== Epoch 22 ====================\n",
      "Train Acc: 0.477992 Train Loss: 1.237593\n",
      "  Val Acc: 0.495349   Val Loss: 1.371615\n",
      "==================== Epoch 24 ====================\n",
      "Train Acc: 0.497297 Train Loss: 1.193875\n",
      "  Val Acc: 0.500000   Val Loss: 1.243437\n",
      "==================== Epoch 26 ====================\n",
      "Train Acc: 0.521236 Train Loss: 1.136424\n",
      "  Val Acc: 0.537209   Val Loss: 1.145917\n",
      "==================== Epoch 28 ====================\n",
      "Train Acc: 0.535135 Train Loss: 1.117783\n",
      "  Val Acc: 0.497674   Val Loss: 1.391769\n",
      "==================== Epoch 30 ====================\n",
      "Train Acc: 0.559073 Train Loss: 1.111592\n",
      "  Val Acc: 0.544186   Val Loss: 1.577553\n",
      "==================== Epoch 32 ====================\n",
      "Train Acc: 0.557529 Train Loss: 1.115582\n",
      "  Val Acc: 0.546512   Val Loss: 1.610425\n",
      "==================== Epoch 34 ====================\n",
      "Train Acc: 0.589189 Train Loss: 1.028575\n",
      "  Val Acc: 0.579070   Val Loss: 1.297681\n",
      "==================== Epoch 36 ====================\n",
      "Train Acc: 0.597683 Train Loss: 1.009853\n",
      "  Val Acc: 0.562791   Val Loss: 1.871596\n",
      "==================== Epoch 38 ====================\n",
      "Train Acc: 0.614672 Train Loss: 0.973733\n",
      "  Val Acc: 0.602326   Val Loss: 1.939616\n",
      "==================== Epoch 40 ====================\n",
      "Train Acc: 0.612355 Train Loss: 0.958523\n",
      "  Val Acc: 0.641860   Val Loss: 1.014772\n",
      "==================== Epoch 42 ====================\n",
      "Train Acc: 0.608494 Train Loss: 0.941193\n",
      "  Val Acc: 0.618605   Val Loss: 1.863895\n",
      "==================== Epoch 44 ====================\n",
      "Train Acc: 0.632432 Train Loss: 0.944141\n",
      "  Val Acc: 0.623256   Val Loss: 1.164604\n",
      "==================== Epoch 46 ====================\n",
      "Train Acc: 0.617761 Train Loss: 0.939771\n",
      "  Val Acc: 0.611628   Val Loss: 1.064717\n",
      "==================== Epoch 48 ====================\n",
      "Train Acc: 0.633977 Train Loss: 0.953216\n",
      "  Val Acc: 0.602326   Val Loss: 1.293494\n",
      "==================== Epoch 50 ====================\n",
      "Train Acc: 0.635521 Train Loss: 0.930280\n",
      "  Val Acc: 0.625581   Val Loss: 1.014296\n"
     ]
    }
   ],
   "source": [
    "################################################################################\n",
    "# You can adjust those hyper parameters to loop for max_epochs times           #\n",
    "################################################################################\n",
    "\n",
    "max_epochs = 50\n",
    "log_interval = 2 # print acc and loss in per log_interval time\n",
    "\n",
    "train_acc_list = []\n",
    "train_loss_list = []\n",
    "val_acc_list = []\n",
    "val_loss_list = []\n",
    "\n",
    "for epoch in range(1, max_epochs + 1):\n",
    "    train_acc, train_loss = train(train_loader, model, criterion, optimizer)\n",
    "    val_acc, val_loss = val(val_loader, model, criterion)\n",
    "\n",
    "    train_acc_list.append(train_acc)\n",
    "    train_loss_list.append(train_loss)\n",
    "    val_acc_list.append(val_acc)\n",
    "    val_loss_list.append(val_loss)\n",
    "    if epoch % log_interval == 0:\n",
    "        print('=' * 20, 'Epoch', epoch, '=' * 20)\n",
    "        print('Train Acc: {:.6f} Train Loss: {:.6f}'.format(train_acc, train_loss))\n",
    "        print('  Val Acc: {:.6f}   Val Loss: {:.6f}'.format(val_acc, val_loss))\n",
    "\n",
    "################################################################################\n",
    "#                               End of your code                               #\n",
    "################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "sdBlib2wzYU7"
   },
   "outputs": [],
   "source": [
    "# save your well-trained state_dict of model\n",
    "torch.save(model.state_dict(), 'NAME_OF_THIS_EXPERIMENT.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W5pW9zcKAN-2"
   },
   "source": [
    "#### Visualize accuracy and loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "4ifzgfp7iq2m"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.plot(range(len(train_loss_list)), train_loss_list)\n",
    "plt.plot(range(len(val_loss_list)), val_loss_list, c='r')\n",
    "plt.legend(['train', 'val'])\n",
    "plt.title('Loss')\n",
    "plt.show()\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.plot(range(len(train_acc_list)), train_acc_list)\n",
    "plt.plot(range(len(val_acc_list)), val_acc_list, c='r')\n",
    "plt.legend(['train', 'val'])\n",
    "plt.title('Acc')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zMYSgO5viYOk"
   },
   "source": [
    "### Predict Result\n",
    "\n",
    "Predict the labesl based on testing set. Upload to [Kaggle](https://www.kaggle.com/c/mis583-2021-flower-classification).\n",
    "\n",
    "**How to upload**\n",
    "\n",
    "1. Click the folder icon in the left hand side of Colab.\n",
    "2. Right click \"result.csv\". Select \"Download\"\n",
    "3. To kaggle. Click \"Submit Predictions\"\n",
    "4. Upload the result.csv\n",
    "5. System will automaticlaly calculate the accuracy of 70% dataset and publish this result to leaderboard.\n",
    "\n",
    "---\n",
    "\n",
    "預測`test`並將結果上傳至Kaggle。[**連結**](https://www.kaggle.com/c/mis583-2021-flower-classification)\n",
    "\n",
    "執行完畢此區的程式碼後，會將`test`預測完的結果存下來。\n",
    "\n",
    "上傳流程\n",
    "1. 點選左側選單最下方的資料夾圖示\n",
    "2. 右鍵「result.csv」\n",
    "3. 點選「Download」\n",
    "4. 至連結網頁點選「Submit Predictions」\n",
    "5. 將剛剛下載的檔案上傳\n",
    "6. 系統會計算並公布其中70%資料的正確率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "mHc4hsPMzYU8"
   },
   "outputs": [],
   "source": [
    "# # if you wanna load previous best model\n",
    "# ckpt = torch.load('NAME_OF_THIS_EXPERIMENT.pt')\n",
    "# model.load_state_dict(ckpt) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "SiyK25P6KXrn"
   },
   "outputs": [],
   "source": [
    "def predict(input_data, model):\n",
    "    model.eval()\n",
    "    output_list = []\n",
    "    with torch.no_grad():\n",
    "        for images in input_data:\n",
    "            images = images.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            output_list.extend(predicted.to('cpu').numpy().tolist())\n",
    "    return output_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "0I0LN7HwpnsX"
   },
   "outputs": [],
   "source": [
    "idx = 0\n",
    "output_csv = predict(test_loader, model)\n",
    "with open('result.csv', 'w', newline='') as csvFile:\n",
    "    writer = csv.DictWriter(csvFile, fieldnames=['file_path', 'label'])\n",
    "    writer.writeheader()\n",
    "    for result in output_csv:\n",
    "        file_path = dataset_test.data_list[idx].replace(data_folder + '/', '')\n",
    "        writer.writerow({'file_path':file_path, 'label':result})\n",
    "        idx += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4XTB6EF6mH2Q"
   },
   "source": [
    "# Keep trying and write report\n",
    "\n",
    "Keep adjust your model, loss function, optimizer...... to train the better model.  \n",
    "Remember record the output of different setting. It's convenient for your to write the report.\n",
    "\n",
    "You are on your way!\n",
    "\n",
    "---\n",
    "\n",
    "持續調整模型、訓練方法、損失函數、優化器等，來訓練出更好的模型，並記錄使用不同參數得出的效果，以利後續 Report 的撰寫。\n",
    "\n",
    "大家加油！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "「Assign-3_flower_classification.ipynb」的副本",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
